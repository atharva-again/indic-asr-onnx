{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f554c9c",
   "metadata": {},
   "source": [
    "# 1. Initial Download and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb18f37",
   "metadata": {},
   "source": [
    "## 1a. Downloading Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c48c1",
   "metadata": {},
   "source": [
    "#### - I am using uv here for package management, because it is faster than pip due to it being rust-based.\n",
    "#### - The full quantization only needs CPU, therefore we will be installing torch and its related libraries for only CPU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cd462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure we are using CPU versions\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "!uv pip uninstall -y onnxruntime-gpu\n",
    "\n",
    "# Install other dependencies\n",
    "!uv pip install onnx torch torchaudio torchcodec --index-url https://download.pytorch.org/whl/cpu\n",
    "!uv pip install datasets pandas huggingface_hub soundfile librosa onnxruntime\n",
    "\n",
    "# Verify CPU environment\n",
    "import onnxruntime as ort\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(f\"Available providers: {ort.get_available_providers()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac60938",
   "metadata": {},
   "source": [
    "## 1b. Setting Up The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3703f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Configuration\n",
    "MODEL_REPO = \"ai4bharat/indic-conformer-600m-multilingual\"\n",
    "LOCAL_MODEL_DIR = \"indic-conformer-600m-onnx\"\n",
    "CALIBRATION_FILE = \"indicvoices_calibration_1408.parquet\"\n",
    "QUANTIZED_MODEL_DIR = \"indic-conformer-600m-quantized-int8\"\n",
    "\n",
    "os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(QUANTIZED_MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5505154",
   "metadata": {},
   "source": [
    "## 1c. Downloading The Model From HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ee045",
   "metadata": {},
   "source": [
    "#### Since the model is gated, model access is needed. Make sure to add your HF read-only access token to Kaggle Secrets for model downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the complete model\n",
    "\n",
    "print(f\"Downloading FULL model from {MODEL_REPO}...\")\n",
    "\n",
    "# The external weight files have NO extension (e.g., \"Constant_1970_attr__value\", \"layers.0.conv.pointwise_conv1.weight\")\n",
    "# We need to download EVERYTHING to get them\n",
    "snapshot_download(\n",
    "    repo_id=MODEL_REPO, \n",
    "    local_dir=LOCAL_MODEL_DIR,\n",
    "    # No allow_patterns = download everything\n",
    "    ignore_patterns=[\".git*\", \"*.md\", \"*.txt\"]  # Ignore only non-essential files\n",
    ")\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check directory structure and verify external weight files\n",
    "\n",
    "print(\"Directory structure:\")\n",
    "for root, dirs, files in os.walk(LOCAL_MODEL_DIR):\n",
    "    level = root.replace(LOCAL_MODEL_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    # Show only ONNX files and important configs\n",
    "    onnx_files = [f for f in files if f.endswith('.onnx') or f.endswith('.json')]\n",
    "    for file in onnx_files[:15]:\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(onnx_files) > 15:\n",
    "        print(f'{subindent}... and {len(onnx_files) - 15} more ONNX/JSON files')\n",
    "\n",
    "# List all ONNX files specifically\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All ONNX files found:\")\n",
    "assets_dir = os.path.join(LOCAL_MODEL_DIR, \"assets\")\n",
    "if os.path.exists(assets_dir):\n",
    "    onnx_files = [f for f in os.listdir(assets_dir) if f.endswith('.onnx')]\n",
    "    for f in sorted(onnx_files):\n",
    "        size = os.path.getsize(os.path.join(assets_dir, f))\n",
    "        print(f\"  {f}: {size / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(\"  assets/ directory not found!\")\n",
    "\n",
    "# Check for external weight files (no extension, large sizes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"External weight files (no extension):\")\n",
    "if os.path.exists(assets_dir):\n",
    "    all_files = os.listdir(assets_dir)\n",
    "    weight_files = [f for f in all_files if '.' not in f or f.startswith('onnx__')]\n",
    "    print(f\"  Found {len(weight_files)} external weight files\")\n",
    "    if weight_files:\n",
    "        # Show first few\n",
    "        for f in sorted(weight_files)[:10]:\n",
    "            size = os.path.getsize(os.path.join(assets_dir, f))\n",
    "            print(f\"    {f}: {size / 1024 / 1024:.2f} MB\")\n",
    "        if len(weight_files) > 10:\n",
    "            print(f\"    ... and {len(weight_files) - 10} more weight files\")\n",
    "        # Total size\n",
    "        total_size = sum(os.path.getsize(os.path.join(assets_dir, f)) for f in weight_files)\n",
    "        print(f\"  Total external weights size: {total_size / 1024 / 1024 / 1024:.2f} GB\")\n",
    "    else:\n",
    "        print(\"  WARNING: No external weight files found! Download may have failed.\")\n",
    "else:\n",
    "    print(\"  assets/ directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7eb16",
   "metadata": {},
   "source": [
    "# 2. Consolidating The Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22bb65b",
   "metadata": {},
   "source": [
    "#### The original model is divided into more than 400 files and the weights are scattered across them. In order to quantize it, we first need to consolidate all external data into their respective models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx.external_data_helper import load_external_data_for_model\n",
    "from onnxruntime.quantization import quant_pre_process, quantize_static, quantize_dynamic, QuantType, QuantFormat, CalibrationMethod\n",
    "import onnxruntime.quantization.calibrate as calibrate_module\n",
    "import onnx\n",
    "import os\n",
    "import gc\n",
    "import functools\n",
    "\n",
    "def consolidate_model(input_filename, output_path, save_external_data=False):\n",
    "    \"\"\"\n",
    "    Consolidates external weights into the model file.\n",
    "    Args:\n",
    "        input_filename: Name of the file in LOCAL_MODEL_DIR or assets/\n",
    "        output_path: Full path for the output file\n",
    "        save_external_data: If True, saves weights to a separate .data file (for >2GB models)\n",
    "    \"\"\"\n",
    "    # Locate input file\n",
    "    input_path = os.path.join(LOCAL_MODEL_DIR, input_filename)\n",
    "    if not os.path.exists(input_path):\n",
    "        input_path = os.path.join(LOCAL_MODEL_DIR, \"assets\", input_filename)\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Skipping {input_filename} (not found)\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Consolidating {input_filename}...\")\n",
    "    model = onnx.load(input_path, load_external_data=False)\n",
    "    base_dir = os.path.dirname(input_path)\n",
    "    load_external_data_for_model(model, base_dir)\n",
    "    \n",
    "    if os.path.exists(output_path): os.remove(output_path)\n",
    "    \n",
    "    if save_external_data:\n",
    "        external_data_file = os.path.basename(output_path) + \".data\"\n",
    "        ext_data_path = os.path.join(os.path.dirname(output_path), external_data_file)\n",
    "        if os.path.exists(ext_data_path): os.remove(ext_data_path)\n",
    "        \n",
    "        onnx.save(model, output_path, \n",
    "                  save_as_external_data=True, \n",
    "                  all_tensors_to_one_file=True, \n",
    "                  location=external_data_file, \n",
    "                  size_threshold=1024, \n",
    "                  convert_attribute=False)\n",
    "    else:\n",
    "        onnx.save(model, output_path)\n",
    "        \n",
    "    print(f\"Saved consolidated model to {output_path}\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    return output_path\n",
    "\n",
    "def quantize_model_helper(input_path, output_path, data_reader=None, quant_type='static'):\n",
    "    \"\"\"\n",
    "    Handles preprocessing and quantization.\n",
    "    Args:\n",
    "        quant_type: 'static' or 'dynamic'\n",
    "    \"\"\"\n",
    "    print(f\"Quantizing {os.path.basename(input_path)} -> {os.path.basename(output_path)} ({quant_type})...\")\n",
    "    \n",
    "    # 1. Preprocess\n",
    "    # We use a temporary path for the preprocessed model\n",
    "    pre_path = input_path + \".pre.onnx\"\n",
    "    \n",
    "    try:\n",
    "        # Always use external data for intermediate preprocessed file to be safe with large models\n",
    "        quant_pre_process(\n",
    "            input_path,\n",
    "            pre_path,\n",
    "            skip_symbolic_shape=True,\n",
    "            save_as_external_data=True,\n",
    "            all_tensors_to_one_file=True,\n",
    "            external_data_location=os.path.basename(pre_path) + \".data\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocessing failed: {e}. Proceeding without preprocessing...\")\n",
    "        pre_path = input_path\n",
    "\n",
    "    # 2. Quantize\n",
    "    if quant_type == 'static':\n",
    "        if data_reader is None:\n",
    "            raise ValueError(\"Data reader required for static quantization\")\n",
    "        \n",
    "        # Monkey-patch onnx.save_model to force external data for the intermediate augmented model\n",
    "        # This fixes InvalidProtobuf error when the augmented model > 2GB\n",
    "        original_save_model = onnx.save_model\n",
    "        original_save = onnx.save\n",
    "        \n",
    "        def patched_save_model(*args, **kwargs):\n",
    "            # Check if this is likely the augmented model (usually in a temp dir or named 'augmented_model.onnx')\n",
    "            # We force external data for ALL saves during this block to be safe\n",
    "            print(f\"DEBUG: Patched save_model called for {args[1] if len(args)>1 else kwargs.get('f')}\")\n",
    "            kwargs['save_as_external_data'] = True\n",
    "            kwargs['all_tensors_to_one_file'] = True\n",
    "            kwargs['size_threshold'] = 1024\n",
    "            # If location isn't provided, generate one based on the filename\n",
    "            if 'location' not in kwargs:\n",
    "                model_path = args[1] if len(args) > 1 else kwargs.get('f')\n",
    "                if isinstance(model_path, str):\n",
    "                    kwargs['location'] = os.path.basename(model_path) + \".data\"\n",
    "            return original_save_model(*args, **kwargs)\n",
    "\n",
    "        # Apply patch to onnx module\n",
    "        onnx.save_model = patched_save_model\n",
    "        onnx.save = patched_save_model\n",
    "        \n",
    "        # Apply patch to calibrate module if it imported save_model directly\n",
    "        original_calibrate_save_model = None\n",
    "        if hasattr(calibrate_module, 'save_model'):\n",
    "            print(\"DEBUG: Patching onnxruntime.quantization.calibrate.save_model\")\n",
    "            original_calibrate_save_model = calibrate_module.save_model\n",
    "            calibrate_module.save_model = patched_save_model\n",
    "        \n",
    "        try:\n",
    "            quantize_static(\n",
    "                model_input=pre_path,\n",
    "                model_output=output_path,\n",
    "                calibration_data_reader=data_reader,\n",
    "                quant_format=QuantFormat.QDQ,\n",
    "                per_channel=True,\n",
    "                reduce_range=False,\n",
    "                activation_type=QuantType.QInt8,\n",
    "                weight_type=QuantType.QInt8,\n",
    "                calibrate_method=CalibrationMethod.MinMax,\n",
    "                use_external_data_format=False, # Output is int8 < 2GB, so single file is fine\n",
    "                extra_options={'ActivationSymmetric': True, 'WeightSymmetric': True}\n",
    "            )\n",
    "        finally:\n",
    "            # Restore original function\n",
    "            onnx.save_model = original_save_model\n",
    "            onnx.save = original_save\n",
    "            if original_calibrate_save_model:\n",
    "                calibrate_module.save_model = original_calibrate_save_model\n",
    "        \n",
    "        # Post-processing: The monkey patch forced the output to be split. \n",
    "        # We now load it and save it again as a single file.\n",
    "        if os.path.exists(output_path):\n",
    "            print(\"DEBUG: Consolidating quantized model into a single file...\")\n",
    "            try:\n",
    "                # Load the model (this loads the external data automatically)\n",
    "                q_model = onnx.load(output_path)\n",
    "                \n",
    "                # Save to a temporary file without external data\n",
    "                temp_output = output_path + \".temp\"\n",
    "                onnx.save_model(q_model, temp_output, save_as_external_data=False)\n",
    "                \n",
    "                # Replace the original file\n",
    "                os.replace(temp_output, output_path)\n",
    "                \n",
    "                # Try to find and remove the .data file created by the patch\n",
    "                # The patch named it os.path.basename(output_path) + \".data\"\n",
    "                data_filename = os.path.basename(output_path) + \".data\"\n",
    "                data_path = os.path.join(os.path.dirname(output_path), data_filename)\n",
    "                if os.path.exists(data_path):\n",
    "                    os.remove(data_path)\n",
    "                    print(f\"DEBUG: Removed external data file: {data_filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: Could not consolidate model: {e}\")\n",
    "            \n",
    "    else:\n",
    "        quantize_dynamic(\n",
    "            model_input=pre_path,\n",
    "            model_output=output_path,\n",
    "            per_channel=True,\n",
    "            weight_type=QuantType.QInt8,\n",
    "            use_external_data_format=False # Output is int8 < 2GB\n",
    "        )\n",
    "        \n",
    "    # Cleanup intermediate files\n",
    "    if pre_path != input_path and os.path.exists(pre_path):\n",
    "        os.remove(pre_path)\n",
    "        if os.path.exists(pre_path + \".data\"):\n",
    "            os.remove(pre_path + \".data\")\n",
    "            \n",
    "    print(f\"Quantization complete: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913311ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is simple verification of consolidated models\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "# Set up execution providers - use CPU for verification to avoid GPU memory issues\n",
    "EXECUTION_PROVIDERS = ['CPUExecutionProvider']\n",
    "print(\"Using CPU for ONNX Runtime verification (to avoid GPU memory issues)\")\n",
    "\n",
    "# Paths to consolidated models\n",
    "encoder_path = os.path.join(LOCAL_MODEL_DIR, \"encoder_consolidated.onnx\")\n",
    "ctc_decoder_path = os.path.join(LOCAL_MODEL_DIR, \"ctc_decoder_consolidated.onnx\")\n",
    "\n",
    "# Load encoder model\n",
    "encoder_sess = ort.InferenceSession(encoder_path, providers=EXECUTION_PROVIDERS)\n",
    "encoder_inputs = encoder_sess.get_inputs()\n",
    "print(\"Encoder input names and shapes:\")\n",
    "for inp in encoder_inputs:\n",
    "    print(f\"  {inp.name}: {inp.shape}, {inp.type}\")\n",
    "\n",
    "# Prepare dummy input for encoder (batch=1, mel_bins=80, T=100)\n",
    "# Note: ONNX model expects float32 inputs; np.random.randn defaults to float64.\n",
    "# Cast explicitly to float32 to avoid \"tensor(double) vs tensor(float)\" errors.\n",
    "dummy_features = np.random.randn(1, 80, 100).astype(np.float32)\n",
    "dummy_length = np.array([100], dtype=np.int64)\n",
    "encoder_input_dict = {encoder_inputs[0].name: dummy_features}\n",
    "if len(encoder_inputs) > 1:\n",
    "    encoder_input_dict[encoder_inputs[1].name] = dummy_length\n",
    "\n",
    "# Run encoder\n",
    "encoder_outputs = encoder_sess.run(None, encoder_input_dict)\n",
    "print(\"Encoder output shapes:\")\n",
    "for out, arr in zip(encoder_sess.get_outputs(), encoder_outputs):\n",
    "    print(f\"  {out.name}: {arr.shape}, {arr.dtype}\")\n",
    "\n",
    "# Load CTC decoder model\n",
    "ctc_sess = ort.InferenceSession(ctc_decoder_path, providers=EXECUTION_PROVIDERS)\n",
    "ctc_inputs = ctc_sess.get_inputs()\n",
    "print(\"CTC Decoder input names and shapes:\")\n",
    "for inp in ctc_inputs:\n",
    "    print(f\"  {inp.name}: {inp.shape}, {inp.type}\")\n",
    "\n",
    "# Prepare dummy input for CTC decoder (use encoder output)\n",
    "ctc_input_dict = {ctc_inputs[0].name: encoder_outputs[0]}\n",
    "if len(ctc_inputs) > 1:\n",
    "    ctc_input_dict[ctc_inputs[1].name] = dummy_length\n",
    "\n",
    "ctc_outputs = ctc_sess.run(None, ctc_input_dict)\n",
    "print(\"CTC Decoder output shapes:\")\n",
    "for out, arr in zip(ctc_sess.get_outputs(), ctc_outputs):\n",
    "    print(f\"  {out.name}: {arr.shape}, {arr.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868241cc",
   "metadata": {},
   "source": [
    "# **3. Quantizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f9f97",
   "metadata": {},
   "source": [
    "## *3a. Defining The Calibration Data Reader*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee299b45",
   "metadata": {},
   "source": [
    "#### We define the class that will work with the Calibration Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377aa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Unified Calibration Data Reader\n",
    "# Set up execution providers\n",
    "print(f\"ONNX Runtime available providers: {ort.get_available_providers()}\")\n",
    "EXECUTION_PROVIDERS = ['CPUExecutionProvider']\n",
    "print(\"Using CPU for ONNX Runtime\")\n",
    "\n",
    "# Mapping from ISO language codes to IndicVoices config names\n",
    "LANG_CODE_MAP = {\n",
    "    'as': 'assamese',\n",
    "    'bn': 'bengali',\n",
    "    'bodo': 'bodo',\n",
    "    'doi': 'dogri',\n",
    "    'gu': 'gujarati',\n",
    "    'hi': 'hindi',\n",
    "    'kn': 'kannada',\n",
    "    'ks': 'kashmiri',\n",
    "    'kok': 'konkani',\n",
    "    'mai': 'maithili',\n",
    "    'ml': 'malayalam',\n",
    "    'mni': 'manipuri',\n",
    "    'mr': 'marathi',\n",
    "    'ne': 'nepali',\n",
    "    'or': 'odia',\n",
    "    'pa': 'punjabi',\n",
    "    'sa': 'sanskrit',\n",
    "    'sat': 'santali',\n",
    "    'sd': 'sindhi',\n",
    "    'ta': 'tamil',\n",
    "    'te': 'telugu',\n",
    "    'ur': 'urdu'\n",
    "}\n",
    "\n",
    "class CalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, parquet_file, model_path, encoder_path=None, batch_size=1, limit=None):\n",
    "        self.df = pd.read_parquet(parquet_file)\n",
    "        if limit:\n",
    "            self.df = self.df.head(limit)\n",
    "        self.data = self.df.to_dict('records')\n",
    "        self.batch_size = batch_size\n",
    "        self.index = 0\n",
    "        \n",
    "        # Preprocessor setup\n",
    "        self.device = 'cpu'\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000, n_fft=512, win_length=400, hop_length=160, \n",
    "            f_min=0.0, f_max=8000.0, n_mels=80, window_fn=torch.hann_window, power=2.0\n",
    "        )\n",
    "        \n",
    "        # Encoder setup (if provided, we run inference on it)\n",
    "        self.encoder_sess = None\n",
    "        if encoder_path:\n",
    "            print(f\"Loading encoder for calibration inference: {encoder_path}\")\n",
    "            self.encoder_sess = ort.InferenceSession(encoder_path, providers=EXECUTION_PROVIDERS)\n",
    "            self.enc_input_name = self.encoder_sess.get_inputs()[0].name\n",
    "            self.enc_len_name = self.encoder_sess.get_inputs()[1].name if len(self.encoder_sess.get_inputs()) > 1 else None\n",
    "\n",
    "        # Session for the model being quantized (to get input names)\n",
    "        session = ort.InferenceSession(model_path, providers=EXECUTION_PROVIDERS)\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.len_input_name = session.get_inputs()[1].name if len(session.get_inputs()) > 1 else None\n",
    "        print(f\"Model inputs: {self.input_name}, {self.len_input_name}\")\n",
    "\n",
    "    def preprocess_audio(self, calibration_row):\n",
    "        # Use speaker_id, duration, lang for robust matching\n",
    "        speaker_id = calibration_row.get('speaker_id')\n",
    "        duration = calibration_row.get('duration')\n",
    "        lang_code = calibration_row.get('lang')\n",
    "        config_name = LANG_CODE_MAP.get(lang_code, lang_code)\n",
    "        \n",
    "        # Note: In a real scenario, loading the dataset streaming for every row is inefficient.\n",
    "        # But keeping logic similar to original for correctness.\n",
    "        ds = load_dataset(\"ai4bharat/IndicVoices\", config_name, split=\"valid\", streaming=True)\n",
    "        \n",
    "        audio_array = None\n",
    "        sample_rate = 16000\n",
    "        \n",
    "        for example in ds:\n",
    "            if (example.get('speaker_id') == speaker_id and\n",
    "                abs(float(example.get('duration', 0)) - float(duration)) < 0.01 and\n",
    "                example.get('lang') == lang_code):\n",
    "                \n",
    "                audio_feat = example[\"audio_filepath\"]\n",
    "                if hasattr(audio_feat, \"get_all_samples\"):\n",
    "                    decoded = audio_feat.get_all_samples()\n",
    "                    audio_array = decoded.data\n",
    "                    sample_rate = decoded.sample_rate\n",
    "                elif hasattr(audio_feat, \"decode\"):\n",
    "                    audio_array, sample_rate = audio_feat.decode()\n",
    "                elif isinstance(audio_feat, dict) and \"array\" in audio_feat:\n",
    "                    audio_array = audio_feat[\"array\"]\n",
    "                    sample_rate = audio_feat[\"sampling_rate\"]\n",
    "                break\n",
    "        \n",
    "        if audio_array is None:\n",
    "            raise FileNotFoundError(f\"Audio not found for {speaker_id}\")\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        if isinstance(audio_array, torch.Tensor):\n",
    "            waveform = audio_array.float()\n",
    "        else:\n",
    "            waveform = torch.from_numpy(audio_array).float()\n",
    "            \n",
    "        if waveform.ndim > 1: waveform = waveform.mean(dim=0)\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "            \n",
    "        features = self.mel_transform(waveform.unsqueeze(0))\n",
    "        features = torch.log(features + 1e-9)\n",
    "        mean = features.mean(dim=2, keepdims=True)\n",
    "        stddev = features.std(dim=2, keepdims=True) + 1e-5\n",
    "        features = (features - mean) / stddev\n",
    "        features = features.squeeze(0)\n",
    "        return features.numpy(), features.shape[1]\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.index >= len(self.data):\n",
    "            return None\n",
    "        batch_rows = self.data[self.index:self.index+self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        row = batch_rows[0]\n",
    "        try:\n",
    "            features, length = self.preprocess_audio(row)\n",
    "            features = np.expand_dims(features, axis=0) # (1, 80, T)\n",
    "            length_arr = np.array([length], dtype=np.int64)\n",
    "            \n",
    "            # If we have an encoder session, run it first (for CTC/Decoder calibration)\n",
    "            if self.encoder_sess:\n",
    "                enc_inputs = {self.enc_input_name: features}\n",
    "                if self.enc_len_name:\n",
    "                    enc_inputs[self.enc_len_name] = length_arr\n",
    "                encoder_outputs = self.encoder_sess.run(None, enc_inputs)\n",
    "                # Return encoder output\n",
    "                return {self.input_name: encoder_outputs[0]}\n",
    "            else:\n",
    "                # Return raw audio features (for Encoder calibration)\n",
    "                inputs = {self.input_name: features}\n",
    "                if self.len_input_name:\n",
    "                    inputs[self.len_input_name] = length_arr\n",
    "                return inputs\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing calibration row: {e}\")\n",
    "            return self.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f48f8",
   "metadata": {},
   "source": [
    "## *3b. Quantizing the Encoder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Quantize Encoder\n",
    "print(\"Processing Encoder...\")\n",
    "\n",
    "# 1. Consolidate (Large model -> keep external data)\n",
    "# encoder_fp32 is the consolidated model path\n",
    "encoder_fp32 = os.path.join(LOCAL_MODEL_DIR, \"encoder_consolidated.onnx\")\n",
    "consolidate_model(\"encoder.onnx\", encoder_fp32, save_external_data=True)\n",
    "\n",
    "# 2. Setup Reader\n",
    "# We pass the FP32 model path to reader so it knows input names\n",
    "data_reader = CalibrationDataReader(CALIBRATION_FILE, encoder_fp32, limit=100)\n",
    "\n",
    "# 3. Quantize (Static PTQ)\n",
    "encoder_int8 = os.path.join(QUANTIZED_MODEL_DIR, \"encoder_quantized_int8.onnx\")\n",
    "quantize_model_helper(encoder_fp32, encoder_int8, data_reader, quant_type='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Quantize CTC Decoder\n",
    "print(\"Processing CTC Decoder...\")\n",
    "\n",
    "# 1. Consolidate (Small model -> single file)\n",
    "# ctc_fp32 is the consolidated model path\n",
    "ctc_fp32 = os.path.join(LOCAL_MODEL_DIR, \"ctc_decoder_consolidated.onnx\")\n",
    "consolidate_model(\"ctc_decoder.onnx\", ctc_fp32, save_external_data=False)\n",
    "\n",
    "# 2. Setup Reader (Needs encoder output)\n",
    "# We reuse the encoder_fp32 from previous step for inference to generate inputs for CTC\n",
    "# Note: We use the FP32 encoder for calibration accuracy\n",
    "encoder_fp32 = os.path.join(LOCAL_MODEL_DIR, \"encoder_consolidated.onnx\")\n",
    "ctc_reader = CalibrationDataReader(CALIBRATION_FILE, ctc_fp32, encoder_path=encoder_fp32, limit=100)\n",
    "\n",
    "# 3. Quantize (Static PTQ)\n",
    "ctc_int8 = os.path.join(QUANTIZED_MODEL_DIR, \"ctc_decoder_quantized_int8.onnx\")\n",
    "quantize_model_helper(ctc_fp32, ctc_int8, ctc_reader, quant_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191f052",
   "metadata": {},
   "source": [
    "## *3c. Quantize RNNT & Joint Modules.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28cf589",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing RNNT & Joint Modules...\")\n",
    "\n",
    "# List of models to process\n",
    "# (Input Filename, Output Filename)\n",
    "models_to_quantize = [\n",
    "    (\"rnnt_decoder.onnx\", \"rnnt_decoder_quantized_int8.onnx\"),\n",
    "    (\"joint_enc.onnx\", \"joint_enc_quantized_int8.onnx\"),\n",
    "    (\"joint_pred.onnx\", \"joint_pred_quantized_int8.onnx\"),\n",
    "    (\"joint_pre_net.onnx\", \"joint_pre_net_quantized_int8.onnx\")\n",
    "]\n",
    "\n",
    "# Add Language Adapters\n",
    "assets_dir = os.path.join(LOCAL_MODEL_DIR, \"assets\")\n",
    "if os.path.exists(assets_dir):\n",
    "    for f in os.listdir(assets_dir):\n",
    "        if f.startswith(\"joint_post_net_\") and f.endswith(\".onnx\"):\n",
    "            lang = f.replace(\"joint_post_net_\", \"\").replace(\".onnx\", \"\")\n",
    "            models_to_quantize.append((f, f\"joint_post_net_{lang}_quantized_int8.onnx\"))\n",
    "\n",
    "for input_name, output_name in models_to_quantize:\n",
    "    # 1. Consolidate (Small -> single file)\n",
    "    # We use a temp path for the consolidated FP32 model\n",
    "    fp32_path = os.path.join(LOCAL_MODEL_DIR, f\"temp_{input_name}\")\n",
    "    \n",
    "    # Consolidate directly to single file (save_external_data=False)\n",
    "    if consolidate_model(input_name, fp32_path, save_external_data=False):\n",
    "        # 2. Quantize (Dynamic PTQ)\n",
    "        int8_path = os.path.join(QUANTIZED_MODEL_DIR, output_name)\n",
    "        quantize_model_helper(fp32_path, int8_path, quant_type='dynamic')\n",
    "        \n",
    "        # Cleanup temp FP32\n",
    "        if os.path.exists(fp32_path): os.remove(fp32_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Merge / Package\n",
    "# Organize files into a clean folder structure before zipping.\n",
    "\n",
    "print(\"Packaging quantized model...\")\n",
    "\n",
    "# Create subdirectories\n",
    "onnx_dir = os.path.join(QUANTIZED_MODEL_DIR, \"onnx\")\n",
    "adapters_dir = os.path.join(onnx_dir, \"adapters\")\n",
    "config_dir = os.path.join(QUANTIZED_MODEL_DIR, \"config\")\n",
    "os.makedirs(onnx_dir, exist_ok=True)\n",
    "os.makedirs(adapters_dir, exist_ok=True)\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "# Move ONNX files to onnx/ (excluding adapters)\n",
    "for f in os.listdir(QUANTIZED_MODEL_DIR):\n",
    "    if f.endswith(\".onnx\") and not f.startswith(\"joint_post_net_\"):\n",
    "        shutil.move(os.path.join(QUANTIZED_MODEL_DIR, f), os.path.join(onnx_dir, f))\n",
    "\n",
    "# Move adapter files to onnx/adapters/\n",
    "for f in os.listdir(QUANTIZED_MODEL_DIR):\n",
    "    if f.startswith(\"joint_post_net_\") and f.endswith(\".onnx\"):\n",
    "        shutil.move(os.path.join(QUANTIZED_MODEL_DIR, f), os.path.join(adapters_dir, f))\n",
    "\n",
    "# Move config files to config/\n",
    "\n",
    "assets_dir = os.path.join(LOCAL_MODEL_DIR, \"assets\")\n",
    "for f in os.listdir(assets_dir):\n",
    "    if f.endswith(\".json\") or f.endswith(\".ts\"):\n",
    "        shutil.copy(os.path.join(assets_dir, f), config_dir)\n",
    "\n",
    "for f in os.listdir(LOCAL_MODEL_DIR):\n",
    "    if f.endswith(\".json\") or f.endswith(\".ts\"):\n",
    "        shutil.copy(os.path.join(LOCAL_MODEL_DIR, f), config_dir)\n",
    "\n",
    "\n",
    "print(f\"Organized files in {QUANTIZED_MODEL_DIR}\")\n",
    "print(f\"Creating zip archive...\")\n",
    "shutil.make_archive(\"indic_conformer_600m_quantized_int8\", 'zip', QUANTIZED_MODEL_DIR)\n",
    "print(f\"Created indic_conformer_600m_int8.zip for HuggingFace upload.\")\n",
    "\n",
    "print(\"Zip file is ready in the working directory. Download it from the 'Output' tab in Kaggle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565dad18",
   "metadata": {},
   "source": [
    "## Transcribe Sample Audio Using Quantized Encoder + CTC Decoder\n",
    "\n",
    "This cell loads a sample WAV file, runs it through the quantized encoder and CTC decoder, and prints the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import onnxruntime as ort\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# ---- CONFIG ----\n",
    "AUDIO_PATH = \"/kaggle/input/gobhi-wav/gobhi.wav\"  # Change to your test file\n",
    "# Update paths to match your local environment if needed\n",
    "ENCODER_ONNX = \"/kaggle/input/indic-conformer-quantized-int8/onnx/default/1/encoder_quantized_int8.onnx\"\n",
    "CTC_ONNX = \"/kaggle/input/indic-conformer-quantized-int8/onnx/default/1/ctc_decoder_quantized_int8.onnx\"\n",
    "# Assuming assets are in the local model dir we created earlier\n",
    "ASSETS_DIR = \"indic-conformer-600m-onnx/assets\" \n",
    "VOCAB_JSON = os.path.join(ASSETS_DIR, \"vocab.json\")\n",
    "MASKS_JSON = os.path.join(ASSETS_DIR, \"language_masks.json\")\n",
    "LANGUAGE_CODE = \"hi\" # 'hi' for Hindi\n",
    "\n",
    "# Helper to find files if paths are wrong (e.g. in Kaggle working dir)\n",
    "def find_file(filename, search_path=\".\"):\n",
    "    if os.path.exists(filename):\n",
    "        return filename\n",
    "    candidates = glob.glob(os.path.join(search_path, \"**\", os.path.basename(filename)), recursive=True)\n",
    "    if candidates:\n",
    "        print(f\"Found {os.path.basename(filename)} at {candidates[0]}\")\n",
    "        return candidates[0]\n",
    "    return None\n",
    "\n",
    "VOCAB_JSON = find_file(VOCAB_JSON) or VOCAB_JSON\n",
    "MASKS_JSON = find_file(MASKS_JSON) or MASKS_JSON\n",
    "\n",
    "# ---- LOAD RESOURCES ----\n",
    "if not os.path.exists(VOCAB_JSON) or not os.path.exists(MASKS_JSON):\n",
    "    raise FileNotFoundError(f\"Missing vocab or masks file. Vocab: {VOCAB_JSON}, Masks: {MASKS_JSON}\")\n",
    "\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_raw = json.load(f)\n",
    "\n",
    "with open(MASKS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    language_masks = json.load(f)\n",
    "\n",
    "print(f\"Loaded vocab for {len(vocab_raw)} languages.\")\n",
    "print(f\"Loaded masks for {len(language_masks)} languages.\")\n",
    "\n",
    "# ---- AUDIO PREPROCESS ----\n",
    "# Ensure audio path exists\n",
    "if not os.path.exists(AUDIO_PATH):\n",
    "    print(f\"Audio file not found at {AUDIO_PATH}. Creating a dummy sine wave for testing...\")\n",
    "    sample_rate = 16000\n",
    "    waveform = torch.sin(torch.linspace(0, 100, 16000*3)).unsqueeze(0) # 3 seconds\n",
    "else:\n",
    "    waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if waveform.ndim > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono (1, T)\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Mel Spectrogram (match calibration: n_mels=80, n_fft=512, win_length=400, hop_length=160)\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=16000, n_fft=512, win_length=400, hop_length=160,\n",
    "    f_min=0.0, f_max=8000.0, n_mels=80, window_fn=torch.hann_window, power=2.0\n",
    ")\n",
    "features = mel_transform(waveform)  # (1, 80, T)\n",
    "features = torch.log(features + 1e-9)\n",
    "# Normalize (per feature)\n",
    "mean = features.mean(dim=2, keepdim=True)\n",
    "stddev = features.std(dim=2, keepdim=True) + 1e-5\n",
    "features = (features - mean) / stddev\n",
    "features = features.squeeze(0)  # (80, T)\n",
    "\n",
    "# ---- ONNX INFERENCE ----\n",
    "features_np = features.numpy().astype(np.float32)\n",
    "length_np = np.array([features_np.shape[1]], dtype=np.int64)\n",
    "encoder_input = {\n",
    "    \"input\": np.expand_dims(features_np, axis=0),  # (1, 80, T)\n",
    "    \"length\": length_np\n",
    "}\n",
    "\n",
    "print(f\"Running inference on {ENCODER_ONNX}...\")\n",
    "encoder_sess = ort.InferenceSession(ENCODER_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "encoder_inputs = encoder_sess.get_inputs()\n",
    "encoder_input_dict = {encoder_inputs[0].name: encoder_input[\"input\"],}\n",
    "if len(encoder_inputs) > 1:\n",
    "    encoder_input_dict[encoder_inputs[1].name] = encoder_input[\"length\"]\n",
    "encoder_outputs = encoder_sess.run(None, encoder_input_dict)\n",
    "\n",
    "print(f\"Running inference on {CTC_ONNX}...\")\n",
    "ctc_sess = ort.InferenceSession(CTC_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "ctc_inputs = ctc_sess.get_inputs()\n",
    "ctc_input_dict = {ctc_inputs[0].name: encoder_outputs[0],}\n",
    "if len(ctc_inputs) > 1:\n",
    "    ctc_input_dict[ctc_inputs[1].name] = encoder_input[\"length\"]\n",
    "ctc_outputs = ctc_sess.run(None, ctc_input_dict)\n",
    "\n",
    "# ---- DECODE WITH MASKS ----\n",
    "logits = ctc_outputs[0]  # (batch, T, 5633)\n",
    "print(f\"Global Logits shape: {logits.shape}\")\n",
    "\n",
    "if LANGUAGE_CODE not in language_masks:\n",
    "    raise ValueError(f\"Language {LANGUAGE_CODE} not found in masks.\")\n",
    "\n",
    "# 1. Get boolean mask\n",
    "mask = np.array(language_masks[LANGUAGE_CODE], dtype=bool)\n",
    "print(f\"Mask length: {len(mask)}, True count: {mask.sum()}\")\n",
    "\n",
    "# 2. Slice logits to get language-specific logits\n",
    "# logits is (1, T, 5633), mask is (5633,)\n",
    "# We want to select columns where mask is True\n",
    "logits_sliced = logits[:, :, mask] # (1, T, 257)\n",
    "print(f\"Sliced Logits shape: {logits_sliced.shape}\")\n",
    "\n",
    "# 3. Argmax to get local indices\n",
    "pred_ids = np.argmax(logits_sliced, axis=-1)[0] # (T,)\n",
    "\n",
    "# 4. Decode using local vocab\n",
    "vocab_list = vocab_raw[LANGUAGE_CODE]\n",
    "BLANK_ID = 256 # Local blank index for Indic Conformer\n",
    "\n",
    "tokens = []\n",
    "prev = None\n",
    "for idx in pred_ids:\n",
    "    if idx == prev:\n",
    "        prev = idx\n",
    "        continue\n",
    "    if idx == BLANK_ID:\n",
    "        prev = idx\n",
    "        continue\n",
    "    \n",
    "    if idx < len(vocab_list):\n",
    "        tokens.append(vocab_list[idx])\n",
    "    prev = idx\n",
    "\n",
    "transcription = \"\".join(tokens).replace(\"â–\", \" \").strip()\n",
    "print(\"-\" * 30)\n",
    "print(\"Transcription:\", transcription)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8f760",
   "metadata": {},
   "source": [
    "## Transcribe Sample Audio Using Quantized Encoder + RNNT Decoder\n",
    "\n",
    "This cell loads a sample WAV file, runs it through the quantized encoder and RNNT decoder (with joint network), and prints the transcription using greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import onnxruntime as ort\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# ---- CONFIG ----\n",
    "AUDIO_PATH = \"/kaggle/input/sample-voices/gobhi.wav\"  # Change to your test file\n",
    "# Update paths to match your local environment if needed\n",
    "ENCODER_ONNX = \"/kaggle/input/indic-conformer-quantized-int8/onnx/default/1/encoder_quantized_int8.onnx\"\n",
    "RNNT_DECODER_ONNX = os.path.join(QUANTIZED_MODEL_DIR, \"rnnt_decoder_quantized_int8.onnx\")\n",
    "JOINT_ENC_ONNX = os.path.join(QUANTIZED_MODEL_DIR, \"joint_enc_int8.onnx\")\n",
    "JOINT_PRED_ONNX = os.path.join(QUANTIZED_MODEL_DIR, \"joint_pred_int8.onnx\")\n",
    "JOINT_PRE_NET_ONNX = os.path.join(QUANTIZED_MODEL_DIR, \"joint_pre_net_int8.onnx\")\n",
    "# Language-specific adapter\n",
    "LANGUAGE_CODE = \"hi\"  # 'hi' for Hindi\n",
    "JOINT_POST_NET_ONNX = os.path.join(QUANTIZED_MODEL_DIR, f\"joint_post_net_{LANGUAGE_CODE}_int8.onnx\")\n",
    "\n",
    "ASSETS_DIR = \"indic-conformer-600m-onnx/assets\" \n",
    "VOCAB_JSON = os.path.join(ASSETS_DIR, \"vocab.json\")\n",
    "MASKS_JSON = os.path.join(ASSETS_DIR, \"language_masks.json\")\n",
    "\n",
    "# Helper to find files if paths are wrong (e.g. in Kaggle working dir)\n",
    "def find_file(filename, search_path=\".\"):\n",
    "    if os.path.exists(filename):\n",
    "        return filename\n",
    "    candidates = glob.glob(os.path.join(search_path, \"**\", os.path.basename(filename)), recursive=True)\n",
    "    if candidates:\n",
    "        print(f\"Found {os.path.basename(filename)} at {candidates[0]}\")\n",
    "        return candidates[0]\n",
    "    return None\n",
    "\n",
    "VOCAB_JSON = find_file(VOCAB_JSON) or VOCAB_JSON\n",
    "MASKS_JSON = find_file(MASKS_JSON) or MASKS_JSON\n",
    "\n",
    "# ---- LOAD RESOURCES ----\n",
    "if not os.path.exists(VOCAB_JSON) or not os.path.exists(MASKS_JSON):\n",
    "    raise FileNotFoundError(f\"Missing vocab or masks file. Vocab: {VOCAB_JSON}, Masks: {MASKS_JSON}\")\n",
    "\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_raw = json.load(f)\n",
    "\n",
    "with open(MASKS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    language_masks = json.load(f)\n",
    "\n",
    "print(f\"Loaded vocab for {len(vocab_raw)} languages.\")\n",
    "print(f\"Loaded masks for {len(language_masks)} languages.\")\n",
    "\n",
    "# ---- LOAD ONNX MODELS ----\n",
    "print(\"Loading ONNX models...\")\n",
    "encoder_sess = ort.InferenceSession(ENCODER_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "rnnt_decoder_sess = ort.InferenceSession(RNNT_DECODER_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "joint_enc_sess = ort.InferenceSession(JOINT_ENC_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "joint_pred_sess = ort.InferenceSession(JOINT_PRED_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "joint_pre_net_sess = ort.InferenceSession(JOINT_PRE_NET_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "joint_post_net_sess = ort.InferenceSession(JOINT_POST_NET_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"Models loaded successfully.\")\n",
    "\n",
    "# Print input shapes for debugging\n",
    "print(\"Joint Enc inputs:\")\n",
    "for inp in joint_enc_sess.get_inputs():\n",
    "    print(f\"  {inp.name}: {inp.shape}\")\n",
    "print(\"Joint Pred inputs:\")\n",
    "for inp in joint_pred_sess.get_inputs():\n",
    "    print(f\"  {inp.name}: {inp.shape}\")\n",
    "print(\"Joint Pre Net inputs:\")\n",
    "for inp in joint_pre_net_sess.get_inputs():\n",
    "    print(f\"  {inp.name}: {inp.shape}\")\n",
    "print(\"Joint Post Net inputs:\")\n",
    "for inp in joint_post_net_sess.get_inputs():\n",
    "    print(f\"  {inp.name}: {inp.shape}\")\n",
    "\n",
    "# ---- AUDIO PREPROCESS ----\n",
    "# Ensure audio path exists\n",
    "if not os.path.exists(AUDIO_PATH):\n",
    "    print(f\"Audio file not found at {AUDIO_PATH}. Creating a dummy sine wave for testing...\")\n",
    "    sample_rate = 16000\n",
    "    waveform = torch.sin(torch.linspace(0, 100, 16000*3)).unsqueeze(0) # 3 seconds\n",
    "else:\n",
    "    waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "if waveform.ndim > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono (1, T)\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Mel Spectrogram (match calibration: n_mels=80, n_fft=512, win_length=400, hop_length=160)\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=16000, n_fft=512, win_length=400, hop_length=160,\n",
    "    f_min=0.0, f_max=8000.0, n_mels=80, window_fn=torch.hann_window, power=2.0\n",
    ")\n",
    "features = mel_transform(waveform)  # (1, 80, T)\n",
    "features = torch.log(features + 1e-9)\n",
    "# Normalize (per feature)\n",
    "mean = features.mean(dim=2, keepdim=True)\n",
    "stddev = features.std(dim=2, keepdim=True) + 1e-5\n",
    "features = (features - mean) / stddev\n",
    "features = features.squeeze(0)  # (80, T)\n",
    "\n",
    "# ---- ENCODER INFERENCE ----\n",
    "features_np = features.numpy().astype(np.float32)\n",
    "length_np = np.array([features_np.shape[1]], dtype=np.int64)\n",
    "encoder_input = {\n",
    "    \"input\": np.expand_dims(features_np, axis=0),  # (1, 80, T)\n",
    "    \"length\": length_np\n",
    "}\n",
    "\n",
    "print(f\"Running inference on {ENCODER_ONNX}...\")\n",
    "encoder_inputs = encoder_sess.get_inputs()\n",
    "encoder_input_dict = {encoder_inputs[0].name: encoder_input[\"input\"],}\n",
    "if len(encoder_inputs) > 1:\n",
    "    encoder_input_dict[encoder_inputs[1].name] = encoder_input[\"length\"]\n",
    "encoder_outputs = encoder_sess.run(None, encoder_input_dict)\n",
    "encoder_output = encoder_outputs[0]  # (1, T, D)\n",
    "\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "\n",
    "# Transpose encoder output to (1, T, D_enc) for joint_enc\n",
    "encoder_output_transposed = encoder_output.transpose(0, 2, 1)  # (1, 69, 1024)\n",
    "\n",
    "# ---- RNNT DECODING (Greedy) ----\n",
    "print(\"Starting RNNT decoding (Greedy Search)...\")\n",
    "\n",
    "# 1. Run Joint Encoder once\n",
    "# encoder_output_transposed: (1, T, 1024)\n",
    "joint_enc_input_dict = {joint_enc_sess.get_inputs()[0].name: encoder_output_transposed}\n",
    "enc_outputs = joint_enc_sess.run(None, joint_enc_input_dict)\n",
    "enc_output = enc_outputs[0]  # (1, T, 640)\n",
    "T = enc_output.shape[1]\n",
    "\n",
    "print(f\"Encoder output processed. Time steps: {T}\")\n",
    "\n",
    "# 2. Decoding Loop\n",
    "BLANK_ID = 256\n",
    "predicted_tokens = [BLANK_ID]  # Start with BLANK/SOS token\n",
    "t = 0\n",
    "max_symbols = 100\n",
    "\n",
    "# Initial Prediction Network run\n",
    "decoder_input = np.array([predicted_tokens], dtype=np.int32)\n",
    "target_length = np.array([len(predicted_tokens)], dtype=np.int32)\n",
    "rnnt_input_dict = {\n",
    "    'targets': decoder_input,\n",
    "    'target_length': target_length,\n",
    "    'states.1': np.zeros((2, 1, 640), dtype=np.float32),\n",
    "    'onnx::Slice_3': np.zeros((2, 1, 640), dtype=np.float32)\n",
    "}\n",
    "decoder_outputs = rnnt_decoder_sess.run(None, rnnt_input_dict)\n",
    "# (1, 640, 1) -> (1, 1, 640)\n",
    "decoder_output = decoder_outputs[0].transpose(0, 2, 1)\n",
    "last_token_embedding = decoder_output[:, -1:, :]\n",
    "\n",
    "joint_pred_input_dict = {joint_pred_sess.get_inputs()[0].name: last_token_embedding}\n",
    "pred_outputs = joint_pred_sess.run(None, joint_pred_input_dict)\n",
    "pred_current = pred_outputs[0] # (1, 1, 640)\n",
    "\n",
    "while t < T and len(predicted_tokens) < max_symbols:\n",
    "    # Get current encoder frame\n",
    "    enc_current = enc_output[:, t:t+1, :] # (1, 1, 640)\n",
    "    \n",
    "    # Combine\n",
    "    joint_input = enc_current + pred_current # (1, 1, 640)\n",
    "    \n",
    "    # Joint Net\n",
    "    joint_pre_input_dict = {joint_pre_net_sess.get_inputs()[0].name: joint_input}\n",
    "    pre_net_output = joint_pre_net_sess.run(None, joint_pre_input_dict)[0]\n",
    "    \n",
    "    joint_post_input_dict = {joint_post_net_sess.get_inputs()[0].name: pre_net_output}\n",
    "    logits = joint_post_net_sess.run(None, joint_post_input_dict)[0] # (1, 1, vocab)\n",
    "    \n",
    "    # Argmax\n",
    "    k = np.argmax(logits[0, 0, :])\n",
    "    \n",
    "    if k == BLANK_ID:\n",
    "        t += 1\n",
    "    else:\n",
    "        predicted_tokens.append(int(k))\n",
    "        # Update Prediction Network\n",
    "        decoder_input = np.array([predicted_tokens], dtype=np.int32)\n",
    "        target_length = np.array([len(predicted_tokens)], dtype=np.int32)\n",
    "        rnnt_input_dict['targets'] = decoder_input\n",
    "        rnnt_input_dict['target_length'] = target_length\n",
    "        \n",
    "        decoder_outputs = rnnt_decoder_sess.run(None, rnnt_input_dict)\n",
    "        decoder_output = decoder_outputs[0].transpose(0, 2, 1)\n",
    "        last_token_embedding = decoder_output[:, -1:, :]\n",
    "        \n",
    "        joint_pred_input_dict = {joint_pred_sess.get_inputs()[0].name: last_token_embedding}\n",
    "        pred_outputs = joint_pred_sess.run(None, joint_pred_input_dict)\n",
    "        pred_current = pred_outputs[0]\n",
    "\n",
    "print(f\"Decoding complete. Tokens: {predicted_tokens}\")\n",
    "\n",
    "# ---- DECODE TOKENS ----\n",
    "vocab_list = vocab_raw[LANGUAGE_CODE]\n",
    "BLANK_ID = 256\n",
    "\n",
    "tokens = []\n",
    "prev = None\n",
    "for idx in predicted_tokens[1:]:  # Skip SOS token\n",
    "    if idx == prev:\n",
    "        prev = idx\n",
    "        continue\n",
    "    if idx == BLANK_ID:\n",
    "        prev = idx\n",
    "        continue\n",
    "    \n",
    "    if idx < len(vocab_list):\n",
    "        tokens.append(vocab_list[idx])\n",
    "    prev = idx\n",
    "\n",
    "transcription = \"\".join(tokens).replace(\"â–\", \" \").strip()\n",
    "print(\"-\" * 30)\n",
    "print(\"RNNT Transcription:\", transcription)\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
